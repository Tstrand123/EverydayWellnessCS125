{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2d5998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hanna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hanna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hanna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hanna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Reference: https://www.kaggle.com/code/artemkalinin/hybrid-recommendation-system-cb-keras/notebook\n",
    "# for uploading a model to tensorflow: https://www.tensorflow.org/guide/keras/save_and_serialize?authuser=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2067c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "meals = pd.read_csv(\"MLData\\dummyDataForApp.csv\")\n",
    "ratings = pd.read_csv(\"MLData\\mealRatings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4af2e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    meal_id                                               NAME  CALORIES  \\\n",
      "0         0                             [ramen, noodle, salad]       292   \n",
      "1         1                               [Chuletas, Guisadas]       516   \n",
      "2         2                                [tortellini, salad]       186   \n",
      "3         3                         [beef, tomato, rice, bowl]       612   \n",
      "4         4                          [peanut, noodle, chicken]       727   \n",
      "5         5                        [southwest, tofu, scramble]       176   \n",
      "6         6                           [Sausage, pepper, pasta]       517   \n",
      "7         7                     [spinach, tortellini, skillet]       648   \n",
      "8         8                                [naked, fish, taco]       293   \n",
      "9         9                     [Blackened, tilaipia, zoodles]       203   \n",
      "10       10                     [Asparagus-mushroom, frittata]       130   \n",
      "11       11                             [sage, rubbed, salmon]       220   \n",
      "12       12                           [chicken, nisoie, salad]       289   \n",
      "13       13                            [Vegetarian, enchilada]       547   \n",
      "14       14                          [lentil, mushroom, gravy]       441   \n",
      "15       15                      [Roasted, cauliflower, salad]       374   \n",
      "16       16                  [tomato, herb, rice, white, bean]       316   \n",
      "17       17                              [purple, power, bowl]       474   \n",
      "18       18                           [Lemon, pepper, chicken]       253   \n",
      "19       19                            [No-bake, granola, bar]       285   \n",
      "20       20                  [Lemon-Broccoli, pasta, Parmesan]       210   \n",
      "21       21                         [Sea, Bass, citrus, salsa]       298   \n",
      "22       22    [sheet, pan, shrimp,, pineapple,, pepper, rice]       505   \n",
      "23       23                          [quick, chicken, fajitas]       413   \n",
      "24       24              [Sriracha-buffalo, cauliflower, bite]        99   \n",
      "25       25  [cheesy, spinach, artichoke, stuffed, spaghett...       223   \n",
      "26       26             [cauliflower, tikka, masala, chickpea]       268   \n",
      "27       27                        [Rotisserie, chicken, taco]       320   \n",
      "28       28             [frittata, asparagus,, leek,, ricotta]       369   \n",
      "29       29                     [Chicken, broccoli, casserole]       584   \n",
      "30       30             [garlic,, sausage,, kale, naan, pizza]       498   \n",
      "31       31                       [chicken, caprese, sandwich]       667   \n",
      "32       32                  [creamy, shrimp, mushroom, pasta]       444   \n",
      "33       33      [Black, Bean, mushroom, enchilada, casserole]       430   \n",
      "34       34                      [vegan, mushroom, stroganoff]       430   \n",
      "35       35                  [skillet, steak, mushroom, sauce]       231   \n",
      "36       36             [cheesy, beef, cauliflower, casserole]       351   \n",
      "37       37                               [breakfast, tostada]       365   \n",
      "38       38                              [oatmeal, fruit, nut]       341   \n",
      "39       39                    [southwest, breakfast, skillet]       341   \n",
      "40       40  [breakfast, salad, egg, salsa, verde, vinaigre...       527   \n",
      "41       41                      [salmon, avacado, poke, bowl]       442   \n",
      "42       42                         [falafel, tabbouleh, bowl]       416   \n",
      "43       43                     [peanut, butter, energy, ball]       174   \n",
      "44       44                    [Blueberry-lemon, energy, ball]       190   \n",
      "45       45                     [baked, zucchini, waffle, fry]        84   \n",
      "46       46                        [crispy, cauliflower, taco]       157   \n",
      "47       47                        [Vegetarian, lettuce, wrap]       178   \n",
      "48       48             [buffalo, chicken, cauliflower, pizza]       189   \n",
      "49       49                                 [chicken, piccata]       249   \n",
      "50       50                 [sweet, sour, pork, sesame, crust]       415   \n",
      "\n",
      "    CARBS  PROTEIN  FAT               MEAL TYPE  \\\n",
      "0      22       17   16             side, lunch   \n",
      "1      11       52   29           dinner, lunch   \n",
      "2      22        7    8                    side   \n",
      "3      78       17   25           dinner, lunch   \n",
      "4      56       34   43           dinner, lunch   \n",
      "5       8       13   11     side, lunch, dinner   \n",
      "6      57       19   24           lunch, dinner   \n",
      "7      77       28   27           lunch, dinner   \n",
      "8       6       33   16           lunch, dinner   \n",
      "9       8       34    4           lunch, dinner   \n",
      "10      5        9    8  side, lunch, breakfast   \n",
      "11      1       19   15           entree, lunch   \n",
      "12      9       24   18     side, lunch, dinner   \n",
      "13     46       26   31           lunch, dinner   \n",
      "14     35       16   28     side, lunch, dinner   \n",
      "15     30       11   24             side, lunch   \n",
      "16     55       11    5     side, lunch, dinner   \n",
      "17     64       16   20           lunch, dinner   \n",
      "18      3       34   10                  entree   \n",
      "19     44        7   10                   snack   \n",
      "20     24        9   10           dinner, lunch   \n",
      "21     14       32   12           dinner, lunch   \n",
      "22     68       26   14           dinner, lunch   \n",
      "23     43       29   16           dinner, lunch   \n",
      "24      8        3    7             snack, side   \n",
      "25     23       10   11           dinner, lunch   \n",
      "26     26        8   16           dinner, lunch   \n",
      "27     34       34    7           dinner, lunch   \n",
      "28     14       18   27        breakfast, lunch   \n",
      "29     45       40   24                  dinner   \n",
      "30     33       30   28           dinner, lunch   \n",
      "31     41       42   38           lunch, dinner   \n",
      "32     50       37   14           lunch, dinner   \n",
      "33     49       13   21                  dinner   \n",
      "34     55       12   18                  dinner   \n",
      "35     18       26    7                  dinner   \n",
      "36     11       26   23                  dinner   \n",
      "37     40       15   19               breakfast   \n",
      "38     47       22    8               breakfast   \n",
      "39     23       17   20               breakfast   \n",
      "40     37       16   34               breakfast   \n",
      "41     34       30   22           lunch, dinner   \n",
      "42     37       11   26                   lunch   \n",
      "43     18        4    9                   snack   \n",
      "44     27        4    9                   snack   \n",
      "45      7        4    5                   snack   \n",
      "46     23        5    5           lunch, dinner   \n",
      "47     11       13   11           lunch, dinner   \n",
      "48      7       21    7           lunch, dinner   \n",
      "49     10       27   10                  entree   \n",
      "50     13       29   28                  entree   \n",
      "\n",
      "                               MAIN FLAVORS  \\\n",
      "0                 [cabbage, ramen, chicken]   \n",
      "1              [pork, tomato, sazon, adobo]   \n",
      "2                 [squash, zuccini, cheese]   \n",
      "3                 [beef, tomato, chickpeas]   \n",
      "4                 [peanut, ginger, chicken]   \n",
      "5                            [tofu, chilli]   \n",
      "6                  [sausage, pasta, pepper]   \n",
      "7          [spinach, creamcheese, tomatoes]   \n",
      "8              [tilapia, coleslaw, avocado]   \n",
      "9              [tilapia, zucchini, paprika]   \n",
      "10           [mushrooms, asparagus, pepper]   \n",
      "11                           [salmon, sage]   \n",
      "12                  [chicken, egg, lettuce]   \n",
      "13                 [spinach, beans, pepper]   \n",
      "14        [mushrooms, lentils, coconutmilk]   \n",
      "15        [cauliflower, paprika, chickpeas]   \n",
      "16                 [tomato, beans, spinach]   \n",
      "17             [chickpeas, cabbage, tahini]   \n",
      "18                 [chicken, lemon, pepper]   \n",
      "19                  [oats, dates, cinnamon]   \n",
      "20              [lemon, broccoli, parmesan]   \n",
      "21            [seabass, orange, grapefruit]   \n",
      "22              [shrimp, pineapple, pepper]   \n",
      "23              [chicken, bellpepper, corn]   \n",
      "24        [cauliflower, sriracha, hotsauce]   \n",
      "25     [spaghettisquash, spinach, parmesan]   \n",
      "26        [cauliflower, cinnamon, turmeric]   \n",
      "27                    [chicken, lime, corn]   \n",
      "28                   [leek, asparagus, egg]   \n",
      "29           [chicken, broccoli, brownrice]   \n",
      "30             [turkey, mozzarella, garlic]   \n",
      "31            [chicken, tomato, mozzarella]   \n",
      "32              [shrimp, mushroom, cashews]   \n",
      "33  [mushrooms, blackbeans, enchiladasauce]   \n",
      "34     [mushrooms, dijonmustard, whitewine]   \n",
      "35             [mushrooms, steak, broccoli]   \n",
      "36                  [beef, jalapeno, chili]   \n",
      "37         [blackbeans, egg, tacoseasoning]   \n",
      "38                     [oat, yogurt, apple]   \n",
      "39              [bacon, potato, bellpepper]   \n",
      "40       [avocado, kidneybeans, saladgreen]   \n",
      "41              [salmon, brownrice, tamari]   \n",
      "42           [falafel, tabbouleh, tzatziki]   \n",
      "43          [peanutbutter, oats, chocolate]   \n",
      "44                   [oats, lemon, walnuts]   \n",
      "45                [zucchini, oldbay, lemon]   \n",
      "46     [cauliflower, refriedbeans, cabbage]   \n",
      "47               [mushrooms, lettuce, tofu]   \n",
      "48          [hotsauce, chicken, bluecheese]   \n",
      "49                 [chicken, lemon, garlic]   \n",
      "50              [pork, soysauce, scallions]   \n",
      "\n",
      "                                   TAGS  \n",
      "0                                [meat]  \n",
      "1                                [meat]  \n",
      "2                   [dairy, vegetables]  \n",
      "3                                [meat]  \n",
      "4                       [peanuts, meat]  \n",
      "5                   [vegetarian, vegan]  \n",
      "6                                [meat]  \n",
      "7                          [vegetarian]  \n",
      "8                          [keto, fish]  \n",
      "9              [keto, vegetables, fish]  \n",
      "10  [keto, vegetarian, vegetables, egg]  \n",
      "11                         [keto, fish]  \n",
      "12                   [keto, vegetables]  \n",
      "13                         [vegetarian]  \n",
      "14                  [vegetarian, vegan]  \n",
      "15             [vegetarian, vegetables]  \n",
      "16                 [vegetarian, lowfat]  \n",
      "17                         [vegetarian]  \n",
      "18                         [keto, meat]  \n",
      "19                         [wholegrain]  \n",
      "20                         [vegetarian]  \n",
      "21                  [keto, fish, fruit]  \n",
      "22             [fish, shellfish, fruit]  \n",
      "23           [meat, highprotein, fruit]  \n",
      "24                  [vegetarian, spicy]  \n",
      "25      [vegetarian, vegetables, dairy]  \n",
      "26             [vegetarian, vegetables]  \n",
      "27           [meat, wholegrain, lowfat]  \n",
      "28  [keto, vegetables, vegetarian, egg]  \n",
      "29             [vegetables, wholegrain]  \n",
      "30            [meat, wholegrain, dairy]  \n",
      "31                   [meat, wholegrain]  \n",
      "32  [nuts, fish, shellfish, wholewheat]  \n",
      "33      [vegetarian, dairy, wholegrain]  \n",
      "34                  [vegan, wholewheat]  \n",
      "35                               [meat]  \n",
      "36            [meat, vegetables, spicy]  \n",
      "37        [egg, vegetarian, wholegrain]  \n",
      "38                  [dairy, vegetarian]  \n",
      "39                          [meat, egg]  \n",
      "40        [vegetables, vegetarian, egg]  \n",
      "41                   [fish, wholegrain]  \n",
      "42                         [vegetarian]  \n",
      "43                [vegetarian, peanuts]  \n",
      "44                  [vegetarian, vegan]  \n",
      "45                    [vegetarian, egg]  \n",
      "46                [vegan, vegetarian, ]  \n",
      "47                [vegetarian, lowcarb]  \n",
      "48                  [spicy, keto, meat]  \n",
      "49                         [keto, meat]  \n",
      "50                               [meat]  \n"
     ]
    }
   ],
   "source": [
    "# Preprocess data -- create a 'bag of words' to find similarity between meals\n",
    "for i in range(0,meals.shape[0]):\n",
    "    # tokenize string to make a list, remove stop words, stemm, etc\n",
    "    meals.at[i, 'MAIN FLAVORS'] = meals.at[i, 'MAIN FLAVORS'].split(',')\n",
    "    meals.at[i, 'TAGS'] = meals.at[i, 'TAGS'].split(',')\n",
    "    \n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return[str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    \n",
    "features = ['MAIN FLAVORS', 'TAGS']\n",
    "\n",
    "for f in features:\n",
    "    meals[f] = meals[f].apply(clean_data)\n",
    "    \n",
    "def processName(x):\n",
    "    lem = WordNetLemmatizer()\n",
    "    #print(x)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.add('ground') # add ground to the stopwords list, want ground beef to just be beef\n",
    "    good_words = []\n",
    "    x = x.split(\" \")\n",
    "    for word in x:\n",
    "        if word not in stop:\n",
    "            if word: # remove any empty strings as a result of \"  \" \n",
    "                good_words.append(lem.lemmatize(word)) # add the base version of the word to the list\n",
    "    return good_words\n",
    "\n",
    "# TODO: spell check\n",
    "# TODO: normalize verious things, IE: wholegrain and wholewheat should be the same\n",
    "meals['NAME'] = meals['NAME'].apply(processName)\n",
    "\n",
    "print(meals)\n",
    "\n",
    "def combine(x):\n",
    "    return ' '.join(x['MAIN FLAVORS']) + ' '+ ' '.join(x['TAGS']) + ' '+ ' '.join(x['NAME'])\n",
    "\n",
    "meals['combinedFeatures'] = meals.apply(combine, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947d20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector of all words\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(meals['combinedFeatures'].values)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107aaf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_feats = pd.DataFrame(x.toarray(), columns=feature_names)\n",
    "meal_feats['combined'] = meal_feats.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "646c4463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(meal_feats)\n",
    "meals['combinedFeatures'] = meal_feats['combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af3c7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the silimarity between THIS meal and all the others, returns the N most similar\n",
    "def get_cossim(meal_id, number):\n",
    "    search = meals[['meal_id', 'combinedFeatures']]\n",
    "    search = search[search.meal_id != meal_id] # remove this meal from the available set (obv will be 1 because they are the same)\n",
    "    search['distance'] = search['combinedFeatures'].apply(lambda x: cosine_similarity(\n",
    "        np.array(x).reshape(1,-1), \n",
    "        np.array(meals.loc[meals['meal_id'] == meal_id]['combinedFeatures'].values[0]).reshape(1,-1)))\n",
    "    search = search.drop(columns=['combinedFeatures']) # don't care about the features in this anymore\n",
    "    search = search.explode('distance').explode('distance') # distance is list of list, this simplifies it\n",
    "    return search.sort_values(by=['distance'], ascending=False)['meal_id'].head(number).values # return the N most similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86191300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates similarity based on user reviews (particularly THIS user's reviews, will need another method to do collaborative filtering)\n",
    "def get_similar(user_id):\n",
    "    rated_meals = ratings[ratings.user_id == user_id] # get a list of all the meals this user has rated\n",
    "    rated_meals = rated_meals[rated_meals['rating'] >= 4.0] # discard everything that's rated less then 4.0 (can change as needed)\n",
    "    top_ratings = (rated_meals.sort_values(by='rating', ascending=False).head(20)) # get the top 20 for this user\n",
    "    top_ratings['rated_meal_id'] = top_ratings['meal_id']\n",
    "    top_ratings = top_ratings[['user_id', 'rated_meal_id']]\n",
    "    top_ratings['similar'] = top_ratings['rated_meal_id'].apply(lambda x: (get_cossim(x, 5))) # find similarity between the top rated meals\n",
    "    result = [x for x in np.concatenate(top_ratings['similar'].values, axis=0).tolist() if x not in top_ratings.rated_meal_id.values.tolist()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62caa4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top(id, top):\n",
    "    # limits results to those with 'top' number of user reviews. Not useful for our dataset at present\n",
    "    similar = get_similar(id)\n",
    "    meal_data = pd.merge(ratings, meals, on='meal_id')\n",
    "    mean_ratings = pd.DataFrame(meal_data.groupby('meal_id')['rating'].mean())\n",
    "    mean_ratings['rating_count'] = pd.DataFrame(meal_data.groupby('meal_id')['rating'].count())\n",
    "    mean_ratings = mean_ratings[mean_ratings['rating_count'] > 10] # gets only those ratings with more then 10 ratings (this won't work for our data, because we don't have that many)\n",
    "    return mean_ratings[mean_ratings.index.isin(similar)].sort_values(by=['rating'],ascending=False).head(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53fa1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = ratings['user_id'].unique().tolist()\n",
    "\n",
    "# creates a mapping between user_ids and a normalized 0-indexed value. IE: removes gaps so 1 = 0, 2 = 1, 5 = 3\n",
    "u2u_enco = {x : i for i, x in enumerate(user_ids)}\n",
    "u_enco2u = {i : x for i, x in enumerate(user_ids)} # reversed mapping so 0 returns 1, 1 returns 2, etc\n",
    "\n",
    "#print(u2u_enco)\n",
    "#print(u_enco2u)\n",
    "\n",
    "meal_ids = ratings['meal_id'].unique().tolist()\n",
    "m2m_enco = {x : i for i, x in enumerate(meal_ids)} # do the same thing for meal id (note: for our datasets, this part is largely unneeded because the meal_ids are already normalized)\n",
    "m_enco2m = {i : x for i, x in enumerate(meal_ids)}\n",
    "\n",
    "ratings[\"user\"] = ratings['user_id'].map(u2u_enco)\n",
    "ratings[\"meal\"] = ratings['meal_id'].map(m2m_enco)\n",
    "\n",
    "num_users = len(u2u_enco)  \n",
    "num_meals = len(m2m_enco)\n",
    "ratings['rating'] = ratings['rating'].values.astype(np.float32)\n",
    "\n",
    "min_rating = min(ratings['rating'])\n",
    "max_rating = max(ratings['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a37ebf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.sample(frac=1, random_state=42)\n",
    "x = ratings[['user', 'meal']].values\n",
    "\n",
    "y = ratings['rating'].apply(lambda x: (x-min_rating)/(max_rating - min_rating)).values\n",
    "\n",
    "training_indices = int(0.9 * ratings.shape[0]) # take 90% for training data\n",
    "x_train, x_val, y_train, y_val = (\n",
    "    x[:training_indices],\n",
    "    x[training_indices:],\n",
    "    y[:training_indices],\n",
    "    y[training_indices:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80ad7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keras part; custom model created through subclassing\n",
    "class RecommenderNet(keras.Model):\n",
    "    def __init__(self, num_users, num_meals, embedding_size, **kwargs):\n",
    "        super(RecommenderNet, self).__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_meals = num_meals\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_embedding = layers.Embedding(num_users, embedding_size, embeddings_initializer='he_normal', embeddings_regularizer=keras.regularizers.l2(1e-6))\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "        self.meal_embedding=layers.Embedding(num_meals, embedding_size, embeddings_initializer='he_normal', embeddings_regularizer=keras.regularizers.l2(1e-6))\n",
    "        self.meal_bias = layers.Embedding(num_meals,1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:,0])\n",
    "        user_bias = self.user_bias(inputs[:,0])\n",
    "        meal_vector = self.meal_embedding(inputs[:,1])\n",
    "        meal_bias = self.meal_bias(inputs[:,1])\n",
    "        dot_user_meal = tf.tensordot(user_vector, meal_vector, 2)\n",
    "        x = dot_user_meal + user_bias + meal_bias\n",
    "        return tf.nn.sigmoid(x)\n",
    "    \n",
    "model = RecommenderNet(num_users, num_meals, 50) # 50 = embedding_size\n",
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(learning_rate=0.0005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e255844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7090 - val_loss: 0.7202\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6711 - val_loss: 0.7202\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6800 - val_loss: 0.7202\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6898 - val_loss: 0.7200\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6848 - val_loss: 0.7198\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6749 - val_loss: 0.7196\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6698 - val_loss: 0.7194\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6723 - val_loss: 0.7192\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6772 - val_loss: 0.7191\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6785 - val_loss: 0.7189\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6753 - val_loss: 0.7188\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6711 - val_loss: 0.7187\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6691 - val_loss: 0.7186\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6701 - val_loss: 0.7185\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6724 - val_loss: 0.7183\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6733 - val_loss: 0.7182\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6723 - val_loss: 0.7180\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6701 - val_loss: 0.7178\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6685 - val_loss: 0.7176\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6684 - val_loss: 0.7174\n"
     ]
    }
   ],
   "source": [
    "# apply the model to the data\n",
    "history = model.fit(x = x_train, y = y_train, batch_size = 64, epochs=15, verbose =1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58602837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training done, now verify with a test\n",
    "user_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d12291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get recommendations based on content\n",
    "top = get_top(user_id, 20)\n",
    "content_recommendation = top.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "398bad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 101ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meal_df = pd.read_csv(\"MLData\\mealRatings.csv\") # have played with the initial ratings df, so refresh it\n",
    "meals_tried = ratings[ratings.user_id == user_id]\n",
    "not_tried = meal_df[~meal_df['meal_id'].isin(meals_tried.meal_id.values)][\"meal_id\"]\n",
    "not_tried = list(set(not_tried).intersection(set(m2m_enco.keys())))\n",
    "\n",
    "not_tried = [[m2m_enco.get(x)] for x in not_tried]\n",
    "user_encoder = u2u_enco.get(user_id)\n",
    "user_meal_array = np.hstack(([[user_encoder]]*len(not_tried), not_tried))\n",
    "\n",
    "# predict ratings for meals not yet tried using the model we made earlier\n",
    "ratings = model.predict(user_meal_array).flatten()\n",
    "#sort/rank\n",
    "top_rating_indices = ratings.argsort()[-20:][::-1]\n",
    "recommended_meal_ids = [m_enco2m.get(not_tried[x][0]) for x in top_rating_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54fbba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=1, user_id=1, meal_id=7, rating=4.5)\n",
      "Pandas(Index=7, user_id=1, meal_id=37, rating=2.0)\n",
      "Pandas(Index=15, user_id=5, meal_id=11, rating=4.5)\n",
      "Pandas(Index=16, user_id=5, meal_id=15, rating=2.0)\n",
      "Pandas(Index=17, user_id=5, meal_id=22, rating=1.5)\n",
      "Pandas(Index=20, user_id=6, meal_id=22, rating=2.0)\n",
      "Pandas(Index=22, user_id=6, meal_id=11, rating=3.0)\n",
      "Pandas(Index=24, user_id=6, meal_id=36, rating=4.0)\n",
      "Pandas(Index=26, user_id=6, meal_id=1, rating=5.0)\n",
      "Pandas(Index=36, user_id=8, meal_id=15, rating=1.0)\n",
      "Pandas(Index=37, user_id=8, meal_id=33, rating=3.0)\n",
      "Pandas(Index=40, user_id=9, meal_id=0, rating=4.0)\n",
      "Pandas(Index=41, user_id=9, meal_id=4, rating=3.5)\n",
      "Pandas(Index=46, user_id=9, meal_id=15, rating=2.0)\n"
     ]
    }
   ],
   "source": [
    "# prints the top rated for this particular user, their history, don't really care about that for this\n",
    "#top_meals_user = (not_tried.sort_values(by=\"rating\", ascending=False)\n",
    " #                .head(10)\n",
    "  #               .meal_id.values)\n",
    "#meal_df_rows = meal_df[meal_df[\"meal_id\"].isin(top_meals_user)]\n",
    "#for row in meal_df_rows.itertuples():\n",
    " #   print(row.name, \":\", row.combinedFeatures)\n",
    " \n",
    "\n",
    "to_rec = random.sample((content_recommendation + recommended_meal_ids), 10)\n",
    "recommended_meals = meal_df[meal_df[\"meal_id\"].isin(to_rec)]\n",
    "for row in recommended_meals.itertuples():\n",
    "    print(row) # currently prints all the things, because not enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852ab5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
